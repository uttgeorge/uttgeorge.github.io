<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>Newton's method & Quasi-Newton method （BFGS ongoing）</title>
<style>
h1,
h2,
h3,
h4,
h5,
h6,
p,
blockquote {
    margin: 0;
    padding: 0;
}
body {
    font-family: "Helvetica Neue", Helvetica, "Hiragino Sans GB", Arial, sans-serif;
    font-size: 14px;
    line-height: 18px;
    color: #fff;
    background-color: #282a36;
    margin: 20px;
}
table {
	margin: 10px 0 15px 0;
	border-collapse: collapse;
}
td,th {	
	border: 1px solid #ddd;
	padding: 3px 10px;
}
th {
	padding: 5px 10px;	
}
a {
    color: #59acf3;word-wrap: break-word; word-break: break-all;
}
a:hover {
    color: #a7d8ff;
    text-decoration: none;
}
a img {
    border: none;
}
img{max-width: 100%;}
p {
    margin-bottom: 9px;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    color: #fff;
    line-height: 36px;
}
h1 {
    margin-bottom: 18px;
    font-size: 26px;
}
h2 {
    font-size: 22px;
}
h3 {
    font-size: 18px;
}
h4 {
    font-size: 16px;
}
h5 {
    font-size: 14px;
}
h6 {
    font-size: 13px;
}
hr {
    margin: 0 0 19px;
    border: 0;
    border-bottom: 1px solid #ccc;
}
blockquote {
    padding: 13px 13px 21px 15px;
    margin-bottom: 18px;
    font-family:georgia,serif;
    font-style: italic;
}
blockquote:before {
    content:"\201C";
    font-size:40px;
    margin-left:-10px;
    font-family:georgia,serif;
    color:#eee;
}
blockquote p {
    font-size: 14px;
    font-weight: 300;
    line-height: 18px;
    margin-bottom: 0;
    font-style: italic;
}
code, pre {
    font-family: Monaco, Andale Mono, Courier New, monospace;
}
code {
	color: #ff4a14;
    padding: 1px 3px;
    font-size: 12px;
    -webkit-border-radius: 3px;
    -moz-border-radius: 3px;
    border-radius: 3px;
}
pre {
    display: block;
    padding: 14px;
    margin: 0 0 18px;
    line-height: 16px;
    font-size: 11px;
    border: 1px solid #bf370f;
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;background-color: #272822;
}
pre code {
    
    color: #f8f8f2;
    font-size: 11px;
    padding: 0;
}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:10px auto;
    }
}
@media print {
	body,code,pre code,h1,h2,h3,h4,h5,h6 {
		color: black;
	}
	table, pre {
		page-break-inside: avoid;
	}
}
figcaption{text-align:center;}

/* PrismJS 1.14.0
https://prismjs.com/download.html#themes=prism-okaidia&languages=markup+css+clike+javascript */
/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
    color: #f8f8f2;
    background: none;
    text-shadow: 0 1px rgba(0, 0, 0, 0.3);
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #f8f8f2;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
    color: #f92672;
}

.token.boolean,
.token.number {
    color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
    color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
    color: #e6db74;
}

.token.keyword {
    color: #66d9ef;
}

.token.regex,
.token.important {
    color: #fd971f;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}

pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

.line-numbers-rows > span {
    pointer-events: none;
    display: block;
    counter-increment: linenumber;
}

.line-numbers-rows > span:before {
    content: counter(linenumber);
    color: #999;
    display: block;
    padding-right: 0.8em;
    text-align: right;
}

</style>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_SVG-full"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>
<style> @media print{ code[class*="language-"],pre[class*="language-"]{overflow: visible; word-wrap: break-word !important;} }</style></head><body><div id="markdown-body-wrap" class="markdown-body">
<h1 id="toc_0">Newton&#39;s method &amp; Quasi-Newton method （BFGS ongoing）</h1>

<h2 id="toc_1">1. Newton&#39;s method</h2>

<p>Similar to the <strong>Gradient Descent</strong>, Newton&#39;s method is an iterative approach to find the point where the derivative equals 0. </p>

<p>The key point of Newton&#39;s method is using quadratic function including 1st derivative and 2nd derivative to approximate the target function.</p>

<h3 id="toc_2"><u><strong>1.1 One Dimensional (one variable) function</strong></u></h3>

<p>The Taylor polynomial of target function at point \(x_0\):<br/>
\[<br/>
f(x)\approx f(x_0)+f&#39;(x_0)(x-x_0)+\frac{1}{2}f&#39;&#39;(x_0)(x-x_0)^2<br/>
\]<br/>
Now, find the point where the derivative equals 0.<br/>
\[<br/>
f&#39;(x)=f&#39;(x_0)+(x-x_0)f&#39;&#39;(x_0)=0<br/>
\]<br/>
Solve the equation and we get:<br/>
\[<br/>
x=x_0-\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}<br/>
\]</p>

<h3 id="toc_3"><u><strong>1.2 High-Dimensional (Multivariate) function</strong></u></h3>

<p>Again, the Taylor polynomial of target function at point \(x_0\):<br/>
\[<br/>
f(X+\Delta X)\approx f(X)+(\Delta X)^T \nabla f(X)+\frac{1}{2}(\Delta X)^T\nabla^2 f(X)(\Delta X)<br/>
\]<br/>
\[<br/>
f(X)\approx f(X_0)+\nabla f(X_0)^T(X-X_0)+\frac{1}{2}(X-X_0)^T\nabla^2 f(X_0)(X-X_0)<br/>
\]<br/>
Now, find the point where the derivative equals 0.<br/>
\[<br/>
\nabla f(X)=\nabla f(X_0)+\nabla^2 f(X_0)(X-X_0)=0<br/>
\]<br/>
Where \(\nabla^2 f(X_0)\) is a <a href="https://github.com/uttgeorge/Machine-Learning-Models/blob/master/Math/Jacobian%20%26%20Hessian%20Matrix.md">Hessian Matrix</a>, denote as \(H\). Solve the equation and we get:<br/>
\[\begin{align*}<br/>
X = X_0 - (\nabla^2 f(X_0))^{-1}(\nabla f(X_0))<br/>
\end{align*}<br/>
\]<br/>
Denote gradient as \(g\), then<br/>
\[<br/>
X = X_0 - H^{-1}g<br/>
\]</p>

<p><strong>This is a set of solutions for a set of linear functions.</strong></p>

<p>Start from the beginning point \(X_0\), iterate the process<br/>
\[<br/>
X_{k+1}=X_{k} - H_k^{-1}g_k<br/>
\]</p>

<h3 id="toc_4">1.3 Line Search</h3>

<h3 id="toc_5">1.4 Pitfalls of Newton&#39;s method</h3>

<ol>
<li>Computational expensive</li>
</ol>

<h2 id="toc_6">2. Quasi-Newton method</h2>

<p>Newton&#39;s method requires computing Hessian Matrix every iteration, then computing a set of equations based on Hessian Matrix. Besides that, Hessian Matrix may not invertible. </p>

<p>A revised approach is Quasi-Newton method. It does not compute Hessian Matrix and its inverse, but builds a symmetric positive-definite matrix that approximate Hessian Matrix. </p>

<p>The Taylor polynomial at \(x_{k+1}\):</p>

<p>\[<br/>
f(X)\approx f(X_{k+1})+\nabla f(X_{k+1})^T(X-X_{k+1})+\frac{1}{2}(X-X_{k+1})^T\nabla^2 f(X_{k+1})(X-X_{k+1})<br/>
\]</p>

<p>\[<br/>
\nabla f(X)\approx \nabla f(X_{k+1})+\nabla^2 f(X_{k+1})(X-X_{k+1})<br/>
\]<br/>
Set \(X=X_k\):<br/>
\[<br/>
\nabla f(X_k)\approx \nabla f(X_{k+1})+\nabla^2 f(X_{k+1})(X_k-X_{k+1})<br/>
\]</p>

<p>\[<br/>
\nabla f(X_{k+1})- \nabla f(X_k)\approx \nabla^2 f(X_{k+1})(X_{k+1}-X_k)<br/>
\]</p>

<p>\[<br/>
g_{k+1}- g_k\approx \nabla^2 f(X_{k+1})(X_{k+1}-X_k)<br/>
\]</p>

<p>If we set:</p>

<p>\[<br/>
s_k=X_{k+1}-X_k\\\\<br/>
y_k=g_{k+1}- g_k<br/>
\]</p>

<p>Then we get:</p>

<p>\[<br/>
y_k \approx H_{k+1}s_k\\\\<br/>
s_k\approx H_{k+1}^{-1} y_k<br/>
\]</p>

<p>And this is called quasi-newton condition.</p>

<h4 id="toc_7"><strong>BFGS</strong> ？？？？？？？？？？</h4>

<p>Build a approximate matrix B of Hessian Matrix:</p>

<p>\[<br/>
B_k \approx H_k<br/>
\]</p>

<p>and iterative update it:</p>

<p>\[<br/>
B_{k+1}=B_k+\Delta B_k<br/>
\]</p>

<p>\(B_0\) is an Identity Matrix. So the problem now is to solve \(\Delta B_k\).</p>

<p>\[<br/>
\Delta B_k = \alpha uu^T+\beta vv^T<br/>
\]</p>

<p>where</p>

<p>\[\begin{align*}<br/>
u&amp;=y_k\\\\<br/>
v&amp;=B_ks_k\\\\<br/>
\alpha&amp;=\frac{1}{y_k^Ts_k}\\\\<br/>
\beta&amp;=-\frac{1}{s_k^TB_ks_k}<br/>
\end{align*}<br/>
\]</p>

<p>\[\begin{align*}<br/>
\Delta B_k=\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}<br/>
\end{align*}<br/>
\]</p>

</div></body>

</html>
